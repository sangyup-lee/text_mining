{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방법3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Korean_movie_reviews_2016.txt', encoding='utf-8') as f:\n",
    "    docs = [doc.strip().split('\\t') for doc in f]\n",
    "    docs = [(doc[0], int(doc[1])) for doc in docs if len(doc) == 2]\n",
    "    texts, labels = zip(*docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [doc.strip().split() for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = []\n",
    "for words in words_list:\n",
    "    total_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "common_words = [ word for word, count in c.most_common(max_features)]\n",
    "# 빈도를 기준으로 상위 10000개의 단어들만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 대해서 index 생성하기\n",
    "words_dic ={}\n",
    "for index, word in enumerate(common_words):\n",
    "    words_dic[word]=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서를 상위 20000개 단어들에 대해서 index 번호로 표현하기\n",
    "filtered_indexed_words = []\n",
    "for review in words_list:\n",
    "    indexed_words=[]\n",
    "    for word in review:\n",
    "        try:\n",
    "            indexed_words.append(words_dic[word])\n",
    "        except:\n",
    "            pass\n",
    "    filtered_indexed_words.append(indexed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_len = 24\n",
    "X = sequence.pad_sequences(filtered_indexed_words, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148845, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "image_size = max_len\n",
    "patch_size = 4  \n",
    "num_patches = max_len // patch_size\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  \n",
    "transformer_layers = 2\n",
    "mlp_head_units = [512, 64]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(max_len,))\n",
    "\n",
    "    data = layers.Embedding(max_features+1, 32)(inputs)\n",
    "    data = layers.Conv1D(patch_size,2,activation='relu',padding='same')(data)\n",
    "    data = tf.expand_dims(data, -1)\n",
    "\n",
    "    patches = Patches(patch_size)(data)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_classifier = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 24, 32)       640032      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 24, 4)        260         ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 24, 4, 1)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " patches (Patches)              (None, None, 16)     0           ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 6, 64)        1472        ['patches[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 6, 64)       128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 6, 64)       66368       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6, 64)        0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 6, 64)       128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6, 128)       8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 6, 128)       0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6, 64)        8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 6, 64)        0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 6, 64)        0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 6, 64)       128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 6, 64)       66368       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 6, 64)        0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 6, 64)       128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6, 128)       8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 6, 128)       0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 6, 64)        8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 6, 64)        0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 6, 64)        0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 6, 64)       128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 384)          0           ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 384)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 512)          197120      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           32832       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            130         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,038,374\n",
      "Trainable params: 1,038,374\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfa.optimizers.AdamW(\n",
    "    learning_rate=0.0001, weight_decay=0.0001\n",
    ")\n",
    "vit_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "checkpoint_filepath = \"./checkpoints/checkpoint_vit3\"\n",
    "mc = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', mode='min',\n",
    "                     save_best_only=True,\n",
    "                    save_weights_only=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "524/524 [==============================] - 30s 53ms/step - loss: 0.7226 - accuracy: 0.5075 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 2/10\n",
      "524/524 [==============================] - 26s 50ms/step - loss: 0.6942 - accuracy: 0.5140 - val_loss: 0.6925 - val_accuracy: 0.5191\n",
      "Epoch 3/10\n",
      "524/524 [==============================] - 27s 51ms/step - loss: 0.6627 - accuracy: 0.5727 - val_loss: 0.3696 - val_accuracy: 0.8460\n",
      "Epoch 4/10\n",
      "524/524 [==============================] - 27s 51ms/step - loss: 0.3233 - accuracy: 0.8778 - val_loss: 0.2706 - val_accuracy: 0.8872\n",
      "Epoch 5/10\n",
      "524/524 [==============================] - 26s 50ms/step - loss: 0.2479 - accuracy: 0.9094 - val_loss: 0.2690 - val_accuracy: 0.8900\n",
      "Epoch 6/10\n",
      "524/524 [==============================] - 26s 50ms/step - loss: 0.2170 - accuracy: 0.9227 - val_loss: 0.2803 - val_accuracy: 0.8922\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = vit_classifier.fit(\n",
    "    x=X_train,\n",
    "    y=y_train_one_hot,\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[mc,es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqS0lEQVR4nO3deXxU9b3/8dcnO5CEJTsJ+yIBAkEWUUvVq1UWK9YFtK1Xb1v9tVdr0dpe9VqvemtrW6t2sdfSXu/t4q3gUqsFRVpRWxckWCDsBAQMBBLClgDZv78/zmQhhhAgMyeTeT8fj3lk5pwzk89xcN75fr/n+x1zziEiIpEryu8CRETEXwoCEZEIpyAQEYlwCgIRkQinIBARiXAxfhdwqlJTU93gwYP9LkNEJKysXLlyn3Mura19YRcEgwcPpqCgwO8yRETCipntONE+dQ2JiEQ4BYGISIRTEIiIRLiwGyMQETkdtbW1FBcXU1VV5XcpQZWQkEBOTg6xsbEdfo6CQEQiQnFxMUlJSQwePBgz87ucoHDOUV5eTnFxMUOGDOnw89Q1JCIRoaqqipSUlG4bAgBmRkpKyim3ehQEIhIxunMINDqdc4yYICgqreAHr21Ey26LiBwvYoLgzU1l/NebW1lY8LHfpYhIBDp48CC/+MUvTvl5M2fO5ODBg51fUAsREwRfOn8I5w5N4cFX1rOj/Ijf5YhIhDlRENTV1bX7vMWLF9OnT58gVeWJmCCIijIenTOe6CjjzoWrqatv8LskEYkgd999N1u3biU/P5/Jkyczbdo0rrjiCkaPHg3AlVdeycSJExkzZgzz589vet7gwYPZt28f27dvJzc3l5tvvpkxY8Zw6aWXcuzYsU6pLaIuH83u04P/nD2WeQtW8cu3t3HrRcP9LklEfPDgK+tYv/twp77m6P7J/Mdnx5xw/yOPPMLatWtZtWoVb775JrNmzWLt2rVNl3k+/fTT9OvXj2PHjjF58mSuvvpqUlJSjnuNLVu28Ic//IFf/epXzJkzhxdeeIEvfvGLZ1x7xLQIGs3O78/l47J4fOlm1u465Hc5IhKhpkyZcty1/j/96U8ZP348U6dO5eOPP2bLli2feM6QIUPIz88HYOLEiWzfvr1TaomoFgF4l1Z998qxFGw/wLwFq/jz1z9FQmy032WJSAi195d7qPTq1avp/ptvvslf/vIX3nvvPXr27MmFF17Y5lyA+Pj4pvvR0dGd1jUUcS0CgD494/jRteMoKq3kkVc3+l2OiESApKQkKioq2tx36NAh+vbtS8+ePdm4cSPvv/9+SGuLuBZBo2kj0rjpvMH877vbuTg3nWkj2vy+BhGRTpGSksL555/P2LFj6dGjBxkZGU37pk+fzlNPPUVubi5nnXUWU6dODWltFm4TrCZNmuQ664tpqmrrufxnf6eiqpYl8z5Nn55xnfK6ItL1bNiwgdzcXL/LCIm2ztXMVjrnJrV1fER2DTVKiI3mibn5lFfW8O8vrdWsYxGJSBEdBABjs3tzx2dGsmhNCX9atdvvckREQi7igwDgqxcMY+KgvnznT2vZdbBzRuFFRMKFggCIjjIen5NPQ4PjroWraWhQF5GIRI6gBoGZTTezTWZWZGZ3n+CYOWa23szWmdn/BbOe9gxM6cn9nx3Ne9vKefqdj/wqQ0Qk5IIWBGYWDTwJzABGA9eb2ehWx4wA7gHOd86NAeYFq56OmDNpAJ8ZncEPX9vEpj1tX+8rItLdBLNFMAUocs5tc87VAM8Cs1sdczPwpHPuAIBzrjSI9ZyUmfH9q/JI7hHDN579B9V19X6WIyIRLDExMWS/K5hBkA20XPy/OLCtpZHASDN7x8zeN7PpQaynQ1IT4/nB1ePYuKeCx5Zu9rscEZGg83tmcQwwArgQyAHeNrM859zBlgeZ2S3ALQADBw4MelEX52Zw/ZSBzH97Gxedlc7UoSknf5KISDvuvvtuBgwYwK233grAAw88QExMDMuWLePAgQPU1tby3e9+l9mzW3ecBF8wg2AXMKDF45zAtpaKgeXOuVrgIzPbjBcMK1oe5JybD8wHb2Zx0Cpu4b5Zuby3dR/fXLiaV+dNIzkhNhS/VkRC4dW7YU9h575mZh7MeOSEu+fOncu8efOagmDhwoUsWbKE22+/neTkZPbt28fUqVO54oorQv7dysHsGloBjDCzIWYWB1wHvNzqmJfwWgOYWSpeV9G2INbUYb3iY3hsbj4lh47x4Mvr/S5HRMLchAkTKC0tZffu3axevZq+ffuSmZnJvffey7hx47jkkkvYtWsXe/fuDXltQWsROOfqzOw2YAkQDTztnFtnZg8BBc65lwP7LjWz9UA98C3nXHmwajpVZw/sy20XDeenbxRxSW46M/Ky/C5JRDpDO3+5B9O1117L888/z549e5g7dy7PPPMMZWVlrFy5ktjYWAYPHtzm8tPBFtQxAufcYmBxq233t7jvgDsDty7p6xeP4M3NZdzzx0ImDupLenKC3yWJSJiaO3cuN998M/v27eOtt95i4cKFpKenExsby7Jly9ixY4cvdWlm8UnERkfx+Nx8qmrr+dbza7QwnYictjFjxlBRUUF2djZZWVl84QtfoKCggLy8PH77298yatQoX+ry+6qhsDAsLZF7Z+Zy/5/W8fv3d3DDuYP9LklEwlRhYfMgdWpqKu+9916bx1VWVoaqJLUIOuqGqYP49Mg0Hl68ga1loXuDRESCTUHQQWbGj64ZR0JsNHcsWEVtfYPfJYmIdAoFwSnISE7g+5/LY03xIX72RpHf5YjIKYqEMb7TOUcFwSmakZfFVWdn8+SyIj7cecDvckSkgxISEigvL+/WYeCco7y8nISEU7u6UYPFp+GBK8awfNt+7lywikW3T6NXvP4zinR1OTk5FBcXU1ZW5ncpQZWQkEBOTs4pPUefYKchOSGWH88Zz/W/ep/vLtrA96/K87skETmJ2NhYhgwZ4ncZXZK6hk7T1KEp3DJtKH/4YCd/3RD6KeEiIp1FQXAG7rx0JKMyk/i3F9ZQXlntdzkiIqdFQXAG4mOieeK6fA4fq+PuFwu79SCUiHRfCoIzNCozmW9PP4ul6/fyXEGx3+WIiJwyBUEn+NL5Qzh3aAoPvrKOneVH/S5HROSUKAg6QVSU8eic8URFGXcsXEV9g7qIRCR8KAg6SXafHvzn7LGs3HGAp97a6nc5IiIdpiDoRLPz+zNrXBaPL93M2l2H/C5HRKRDFASdyMx4+MqxpCTGMW/BKqpq6/0uSUTkpBQEnaxPzzgevXY8RaWVPPLqRr/LERE5KQVBEEwbkcZN5w3mf9/dzt+2dO91TUQk/CkIguTuGaMYltaLu55bzcGjNX6XIyJyQgqCIEmIjeYn102gvLKG+15aq1nHItJlKQiCaGx2b+74zEj+vKaEl1fv9rscEZE2KQiC7P99eigTB/XlvpfWsuvgMb/LERH5BAVBkMVER/HYnPE0NDjuWriaBs06FpEuRkEQAoNSenH/Z0fz3rZynn7nI7/LERE5joIgROZMGsAluRn8cMkmNu2p8LscEZEmCoIQMTMeuTqP5IQY5i1YRXWdZh2LSNegIAih1MR4fnD1ODaUHOaxpZv9LkdEBFAQhNzFuRlcP2Ug89/exvJt5X6XIyKiIPDDfbNyGdivJ3cuXE1FVa3f5YhIhFMQ+KBXfAyPzcmn5NAxHnh5vd/liEiEUxD4ZOKgvtx20XBe+LCYVwtL/C5HRCKYgsBHX794BONyenPvHwspPVzldzkiEqEUBD6KjY7i8bn5HKut51vPr9HCdCLii6AGgZlNN7NNZlZkZne3sf8mMyszs1WB21eCWU9XNCwtkXtn5vLW5jJ+v3yn3+WISAQKWhCYWTTwJDADGA1cb2aj2zh0gXMuP3D7dbDq6cpumDqIT49M4+FF69laVul3OSISYYLZIpgCFDnntjnnaoBngdlB/H1hy8z40TXjSIiN5s4Fq6itb/C7JBGJIMEMgmzg4xaPiwPbWrvazNaY2fNmNqCtFzKzW8yswMwKysq651c/ZiQn8L3P5bG6+BA/e6PI73JEJIL4PVj8CjDYOTcOWAr8pq2DnHPznXOTnHOT0tLSQlpgKM3My+Kqs7N5clkRH+484Hc5IhIhghkEu4CWf+HnBLY1cc6VO+eqAw9/DUwMYj1h4YErxpCZnMCdC1ZxtKbO73JEJAIEMwhWACPMbIiZxQHXAS+3PMDMslo8vALYEMR6wkJyQiw/njOeHfuP8t1FEf+fQ0RCIGhB4JyrA24DluB9wC90zq0zs4fM7IrAYbeb2TozWw3cDtwUrHrCydShKdwybSj/t3wnb2zc63c5ItLNWbhNYpo0aZIrKCjwu4ygq66rZ/bP32FfZQ1L5k0jJTHe75JEJIyZ2Urn3KS29vk9WCwnEB8TzRPX5XP4WC33vFioWcciEjQKgi5sVGYy37rsLF5fv5fnVhb7XY6IdFMKgi7uy58awrlDU3jw5XXsLD/qdzki0g0pCLq4qCjj0TnjiTLjzoWrqG9QF5GIdC4FQRjI7tODh64cQ8GOAzz11la/yxGRbkZBECauzM9m1rgsHl+6mbW7Dvldjoh0IwqCMGFmPHzlWFIS45i3YBVVtfV+lyQi3YSCIIz06RnHo9eOp6i0kh+8ttHvckSkm1AQhJlpI9K46bzB/M872/n7ln1+lyMi3YCCIAz92/RRDEvrxV3Prebg0Rq/yxGRMKcgCEM94qJ5Yu4E9lVW850/rfO7HBEJcwqCMJWX05t5l4zgldW7+dOqXSd/gojICSgIwthXLxjGxEF9ue+ltew+eMzvckQkTCkIwlhMdBSPzRlPfYPjrudW06BZxyJyGhQEYW5QSi/uv3w0724t5+l3PvK7HBEJQwqCbmDu5AFckpvBD5dsYtOeCr/LEZEwoyDoBsyMR67OIzkhhnkLVlFdp1nHItJxCoJuIjUxnkeuGseGksM8vnSL3+WISBhREHQjl4zO4PopA/jl21v54KP9fpcjImFCQdDN3DdrNAP79eSOBauoqKr1uxwRCQMKgm6mV3wMj83Jp+TQMR58Zb3f5YhIGFAQdEMTB/Xl1ouG8/zKYl5bW+J3OSLSxSkIuqnbLx5BXnZv7nmxkNLDVX6XIyJdmIKgm4qNjuLxufkcq63n2y+swTnNOhaRtikIurHh6YncOzOXNzeV8fvlO/0uR0S6KAVBN3fD1EF8emQaDy9az7aySr/LEZEuSEHQzZkZP7pmHAmx0dyxYBW19Q1+lyQiXYyCIAJkJCfwvc/lsbr4ED9/o8jvckSki+lQEJjZN8ws2Tz/bWYfmtmlwS5OOs/MvCyumpDNz5cV8Y+dB/wuR0S6kI62CL7knDsMXAr0BW4AHglaVRIUD8weQ2ZyAncuXM3Rmjq/yxGRLqKjQWCBnzOB3znn1rXYJmEiOSGWH88Zz/byIzy8aIPf5YhIF9HRIFhpZq/jBcESM0sCNOoYhqYOTeHmaUN5ZvlO3ti41+9yRKQL6GgQfBm4G5jsnDsKxAL/ErSqJKi+eelIRmUm8e3nCymvrPa7HBHxWUeD4Fxgk3PuoJl9EbgPOHSyJ5nZdDPbZGZFZnZ3O8ddbWbOzCZ1sB45A/Ex0TxxXT6Hj9Vyz4uFmnUsEuE6GgT/BRw1s/HAN4GtwG/be4KZRQNPAjOA0cD1Zja6jeOSgG8Ay0+hbjlDozKT+dZlZ/H6+r08t7LY73JExEcdDYI65/3ZOBv4uXPuSSDpJM+ZAhQ557Y552qAZwPPb+0/gR8AWhktxL78qSFMHdqPB19ex8f7j/pdjoj4pKNBUGFm9+BdNrrIzKLwxgnakw183OJxcWBbEzM7GxjgnFvU3guZ2S1mVmBmBWVlZR0sWU4mKsp49NrxRJlxx4JV1Deoi0gkEnU0COYC1XjzCfYAOcCPzuQXB8LkMbyupnY55+Y75yY55yalpaWdya+VVnL69uShK8dQsOMAv3x7q9/liIgPOhQEgQ//Z4DeZnY5UOWca3eMANgFDGjxOCewrVESMBZ408y2A1OBlzVgHHpX5mczKy+Lx5duZu2uk14DICLdTEeXmJgDfABcC8wBlpvZNSd52gpghJkNMbM44Drg5cadzrlDzrlU59xg59xg4H3gCudcwWmch5wBM+Phz42lb8847liwiqraer9LEpEQ6mjX0L/jzSG40Tn3z3gDwd9p7wnOuTrgNmAJsAFY6JxbZ2YPmdkVZ1K0dL4+PeP40bXj2VJayQ9f2+R3OSISQjEdPC7KOVfa4nE5HQgR59xiYHGrbfef4NgLO1iLBMkFI9O48dxBPP3OR/zTqHQ+NSLV75JEJAQ62iJ4zcyWmNlNZnYTsIhWH/DSPdw9I5dhab2467nVHDpa63c5IhICHR0s/hYwHxgXuM13zv1bMAsTf/SIi+aJuRPYV1nNd/601u9yRCQEOto1hHPuBeCFINYiXUReTm/mXTKCR1/fzMW56czOzz75k0QkbLXbIjCzCjM73MatwswOh6pICb2vXjCMswf24TsvrWX3wWN+lyMiQdRuEDjnkpxzyW3ckpxzyaEqUkIvJjqKx+fmU9fguOu51TRo1rFIt6XvLJYTGpTSi/svH827W8v5n3e3+12OiASJgkDaNXfyAC7JzeAHr21k054Kv8sRkSBQEEi7zIxHrs4jKT6GeQtWUV2nWcci3Y2CQE4qNTGeH1w9jg0lh3niL1v8LkdEOpmCQDrkktEZXD9lAE+9tZUPPtrvdzki0okUBNJh980azYC+Pblz4SoqqjTrWKS7UBBIh/WKj+HxufnsPniMB19Z73c5ItJJFARySiYO6sutFw3n+ZXFvLa2xO9yRKQTKAjklN1+8Qjysntzz4uFlFboq6ZFwp2CQE5ZbGDW8dGaer79/Bqc06xjkXCmIJDTMjw9kXtn5vLmpjKeWb7T73JE5AwoCOS03TB1ENNGpPLwog1sK6v0uxwROU0KAjltUVHGo9eOJy4mijsWrKK2vsHvkkTkNCgI5IxkJCfwvc/lsbr4EE8uK/K7HBE5DQoCOWOzxmVx1YRsfvZGEf/YecDvckTkFCkIpFM8MHsMmckJ3LlwNUdr6vwuR0ROgYJAOkVyQiw/njOe7eVHeHjRBr/LEZFToCCQTjN1aAo3TxvKM8t3smxjqd/liEgHKQikU33z0pGMykziW8+vYf+RGr/LEZEOUBBIp4qPiebxufkcPlbLPS9q1rFIOFAQSKfLzUrmrstGsmTdXp5fWex3OSJyEpETBAd3wscfQK0WSQuFr3xqKFOH9uPBV9bz8f6jfpcjIu2InCBYsxD++zPw/RyYfxEs/hasXgDlW0HdF52ucdaxAXcuXEV9g/4bi3RVMX4XEDJn3whpo2BXARQXwD+egQ/me/t69IOcSZA9CXImQvZE6NHX33q7gZy+PXlw9hjuXLia+W9v42sXDvO7JBFpQ+QEQWIa5F7u3QAa6qFsIxSv8IKhuAC2LAUCf7mmjICcyYFgmAQZYyA61rfyw9XnJmTz1w2lPLZ0E9NGpDI2u7ffJYlIKxZuV3VMmjTJFRQUBOfFqw7D7n944bBrpffzSJm3L6YH9M9v0XKYDL2zg1NHN3PgSA2XPfE2vXvE8srXP0VCbLTfJYlEHDNb6Zyb1OY+BUE7nPMGmZuCoQBKVkN9tbc/KcvrRsqZ7AVE/wkQ1ys0tYWZtzaXcePTH/Cl84dw/2dH+12OSMRpLwgip2vodJhB30HeLe8ab1tdDewtbO5O2lUAG/8cOD4K0sd43Uk5k72WQ+pIiIqcMfkTuWBkGjeeO4in3/mIi3PTOX94qt8liUhAUFsEZjYd+AkQDfzaOfdIq/1fBW4F6oFK4Bbn3Pr2XjOkLYKOOlLe3JW0qwCKV0L1IW9ffDJkn93cnZQzCXpF5ofgsZp6Lv/Z3zhaU89r3/g0vXtqzEUkVHzpGjKzaGAz8BmgGFgBXN/yg97Mkp1zhwP3rwD+1Tk3vb3X7ZJB0FpDA5QXBUIhMBi9dx24em9/38GBYAiEQ2YexMT7WnKorCk+yFW/eJeZeVn89PoJfpcjEjH86hqaAhQ557YFingWmA00BUFjCAT0oumSnTAXFQVpI71b/ue9bTVHoWRVczDsfA/WPu/ti46DzHHNwZA90QsLM7/OIGjG5fThGxeP4MdLN3Nxbjqz8zXgLuK3YAZBNvBxi8fFwDmtDzKzW4E7gTjgn9p6ITO7BbgFYODAgZ1eaEjE9YRB53m3Rod3B8YaAoPRH/4Wlj/l7euZ2uIKpUle91JC97j08msXDmPZplK+89JaJg/uR/8+PfwuSSSiBbNr6BpgunPuK4HHNwDnOOduO8Hxnwcuc87d2N7rhkXX0Omqr4PS9c2T3ooLYN+mwE7zBp4b5zbkTIa0XIgOz/H+7fuOMPOnf2PCwD787kvnEBXV/Vo/Il2JX2ME5wIPOOcuCzy+B8A59/0THB8FHHDOtftnb7cOgrYcOwi7P/QGoBsHo4+We/tie0L/s5snveVMhuQsX8s9FX/4YCf3vFjIuUNTmJ3fn0vHZNKvV5zfZYl0S34FQQzeYPHFwC68weLPO+fWtThmhHNuS+D+Z4H/OFGhjSIuCFpzDg585AVD42B0yRpoqPX2J2cf36WUle91S3VBzjn+662tLFjxMTvKjxIdZZw7NIWZeVlcNiaDlMTIGEAXCQXfJpSZ2UzgCbzLR592zj1sZg8BBc65l83sJ8AlQC1wALitZVC0JeKDoC21VbCn8PirlA7u8PZZtLc8RuOlq9mTIGV4l5rb4Jxj3e7DvLq2hMWFe/ho3xGio4ypQ/sFQiGTVIWCyBnRzOJIVFnWYqxhBez6EGoqvH0JvZtnRDe2HHr287feAOccG0oqWFxYwuLCErbtO0KUwTlDUpg5LovpYzJJS1IoiJwqBYF4cxv2bW4x6a3AG5h2Dd7+fkNbTHqbCBl5EONvf71zjo17Kni1sIRFhSVsLfNCYcoQr6UwfWwm6UkJvtYoEi4UBNK26kpvkb2WVylV7vH2RcdD1vjjV2DtM9C3uQ3OOTbvrWRRoKVQVFqJGUwe3I9ZeVnMGJtJerJCQeREFATSMc7B4V3N4wy7VnpBURf4Vrde6YFJb4GxhuyzIT7Jl1K37K1oCoXNe71QmDSoLzPzspgxNovM3goFkZYUBHL66mu95TFaLs1dXhTYaZCee/wKrGmjICq0y0wXlVawuHAPiwtL2LjHGweZNKgvM/KymJmXSVZvTVgTURBI5zq6PzC3ocUKrMcOePviEr3luJuWy5gESRkhK21rWSWL13hjCo2hcPbAPszMy2JmXpZmMUvEUhBIcDkH+7e1uEKpwLuctaHO29974PGT3rLGQWzwP5C3lVXy6to9LFpTwvoSb1mr/AF9vDGFvExy+nbN+RUiwaAgkNCrPeZNdGu5NPehnd6+qBhvxdXGS1cTMyAmwbtKKSYhcIs//mdUzBkNVG/fd6RpTGHdbi8Uxg/ow6y8TGaMzWJAP4WCdG8KAukaKvYeP+lt9z+gprJjz7UoLxCi49oOijZ/tr2vvNpYXVJFQfERivbXUk0c2al9mDQ8i3NHZpOV0qfFcwK/Lzq+S03CEzlVCgLpmhrqYd8Wb3yhrgrqa7yfddWtfrbeVt3GMa1+tn6t+pozr7e9EIo+cfB8IlTaO+YTv6PlvvBcYDBsOOf9m3T1XrdmQ533+LhtgZ+uodXj+uZjmx7XefN3jnt8Jq9XD6Nnw4App3V6+qpK6ZqioiF9VGh+V0OD913T7YRK2YFDfPjRHgq3l7Jn/0HiqWVQ72jGZiSQmxZH37iG9gOo5oi3IOCJjjnTr9uw6FaBcpLgOaVwahU8UTGh/5Dz+/UbJ1d2NVExga7RaG8F4tMMgvYoCCQyREVBVI92B6nThsBlZ8NlQPGBo7y2dg+LCkv43oaDsAFys5KZlZfJzIlZDE1LPLXf75x3KW59W62ZU2nxtNPqqauG6ooTv1bjwoRdScsPuaiYwPvU3uNo73bc45jmcaTGxxbV6nF083NP5/VP+/XO9HeEpjtSXUMiJ7H74DFeXevNU1i5w7tMdlRmUuDqoyyGp59iKPilob7tgGkrnOprQ/BBqjGXUNIYgUgnKTl0jFcDk9cKAqFwVkYSM/OymDUuk+Hp/sy0FjkZBYFIEOw5VMVrgaWzV+zYj3MwIj0xEApZjMxQKEjXoSAQCbK9h6uaxhRWbPdCYXggFGbmZXJWRhLm04J9IqAgEAmp0ooqlgRC4YOP9tPgYGhaL2YFlrkYlalQkNBTEIj4pKyimtfW7eHVwhLe31buhUJqL2bkZTIzL4vRWckKBQkJBYFIF7Cvspol67yB5ve2eqEwOKVn04J4Y/orFCR4FAQiXUx5ZTWvr9/L4sIS3t1aTn2DY1BKT2aMzWJWXhZjsxUK0rkUBCJd2P4jNby+zhtTaAyFAf16MHOs11IYl9NboSBnTEEgEiYOHKlh6fq9LCos4Z2ifdQ1OHL69gh881om+QP6KBTktCgIRMLQwaM1vL5+L68WlvD3on3U1juy+/RgxthMZo7LYoJCQU6BgkAkzB06WsvSDd6Ywt+2lFFb7+jfO4HpY70ZzRMG9CUqSqEgJ6YgEOlGDh2r5a+BUHh78z5q6hvITE5gRl4ms/KyOHugQkE+SUEg0k0drmoMhT28tbmMmroGMpLjmREYaJ40SKEgHgWBSASoqKrljY2lLFpTwpuBUEhPivfGFPKymDS4H9EKhYilIBCJMJXVdbyxsZTFa0pYtqmU6roG0pLimT7GC4UpQxQKkUZBIBLBjjSGQqEXClW1DaQmxnHZGG9MYcqQfsRE67sBujsFgYgAcLSmjmUby1hcWMIbG0s5VltPSq84LhvrhcI5CoVuS0EgIp9wtKaONzc1h8LRmnr69YrjsjEZXJKbwZj+vclIjtdchW5CQSAi7TpWU89bm0tZVLiHv27Yy9GaegD69owlNyuZUZnJ5GYlkZuVzPD0RBJio32uWE5Ve0GgL68XEXrERTN9bBbTx2ZRVVvP6o8PsqHkMBv3VLCh5DD/98EOqmobAIiOMoal9TouIEZnJZOWpNZDuFIQiMhxEmKjOWdoCucMTWnaVt/g2F5+hA0lh72AKKlgxUf7+dOq3U3H9OsV57UaMpMZleUFxPD0ROJj1Hro6hQEInJSXisgkWFpiVw+rn/T9oNHa5paDY0tiN+9v4PqOq/1EBN4Xm5WUiAcvIBIT0rw61SkDUENAjObDvwEiAZ+7Zx7pNX+O4GvAHVAGfAl59yOYNYkIp2nT884pg5NYWqL1kNdfUOg9dAcEMs/2s9LLVoPqYlxTd1K3k9v7CEuRlcs+SFog8VmFg1sBj4DFAMrgOudc+tbHHMRsNw5d9TMvgZc6Jyb297rarBYJDwdOFLDhj1et9KGksNs2HOYzXsrqWnRehientjUamgMiLSkeJ8r7x78GiyeAhQ557YFingWmA00BYFzblmL498HvhjEekTER317xXHesFTOG5batK2uvoGP9h1hfYuB6fe2lvPHf+xqOiY1Mb7piqXGgBiWptZDZwpmEGQDH7d4XAyc087xXwZebWuHmd0C3AIwcODAzqpPRHwWEx3FiIwkRmQkMbvF9v1HathYcpgNLcYf/ved7dTUe62H2GhjeHoSuZmNAZHMqKwkUhPVejgdXWKw2My+CEwCLmhrv3NuPjAfvK6hEJYmIj7o1yuO84anct7w5tZDbaD14AWDFxB/L9rHiy1aD2lJ8V4wtAiIoWm9iNVs6XYFMwh2AQNaPM4JbDuOmV0C/DtwgXOuOoj1iEgYi42OYmRGEiMzkpid37y9vLK6xZVL3s//2Vre1HqIi446buzBm/+QRIpaD02CGQQrgBFmNgQvAK4DPt/yADObAPwSmO6cKw1iLSLSTaUkxnP+8HjOb9V62FbWPO9hw54K3t5SxgsfFjcdk97YemgREENTe0XkWktBCwLnXJ2Z3QYswbt89Gnn3DozewgocM69DPwISASeC8xI3OmcuyJYNYlIZIiNjuKszCTOykziygnZTdv3VVY3X7UUCIh3t26jtt7rcY6LiWJEoPUwKtObMZ2blUzfXnF+nUpIaK0hEYloNXUNbC2rZOOewy3mPlSwr7K5pzojubn10BgQQ8Ks9aC1hkRETiAuJqrpQ/5zE5q3l1VUB8KhxeD0ln3UNTS3HkZmJJKb2XzV0uisZPr0DL/Wg4JARKQNaUnxpCWlMW1EWtO2mroGikorjwuINzaW8tzK5rGHrN4JjGpx1VJuVhJDUhO79DfCKQhERDooLiaK0f2TGd0/+bjtpRVVbCip8OY+BALiby1aD/Ex3pjFcQGRmUzvnrF+nMYnKAhERM5QelIC6UkJXDCyufVQXVdPUWllc0DsOcxfNpSysKC59dC/d0JTt1JjQAxO6RXy1oOCQEQkCOJjohnTvzdj+vdu2uaco6yimvWBVkNjF9Obm8uoD7QeEmKjOCujeb6DFxTJ9O4RvNaDgkBEJETMjPTkBNKTE7jwrPSm7VW1ja2H5jWXlqzbw7Mrmlfpye7Tg29PP4vZ+dltvfQZURCIiPgsITaasdm9GZt9fOuhtKn14K3amhak2dAKAhGRLsjMyEhOICM5gYtatB6CIXxmQ4iISFAoCEREIpyCQEQkwikIREQinIJARCTCKQhERCKcgkBEJMIpCEREIlzYfTGNmZUBO07z6anAvk4sx086l66nu5wH6Fy6qjM5l0HOubS2doRdEJwJMys40Tf0hBudS9fTXc4DdC5dVbDORV1DIiIRTkEgIhLhIi0I5vtdQCfSuXQ93eU8QOfSVQXlXCJqjEBERD4p0loEIiLSioJARCTCdcsgMLPpZrbJzIrM7O429seb2YLA/uVmNtiHMjukA+dyk5mVmdmqwO0rftR5Mmb2tJmVmtnaE+w3M/tp4DzXmNnZoa6xozpwLhea2aEW78n9oa6xI8xsgJktM7P1ZrbOzL7RxjFh8b508FzC5X1JMLMPzGx14FwebOOYzv0Mc851qxsQDWwFhgJxwGpgdKtj/hV4KnD/OmCB33WfwbncBPzc71o7cC6fBs4G1p5g/0zgVcCAqcByv2s+g3O5EPiz33V24DyygLMD95OAzW38+wqL96WD5xIu74sBiYH7scByYGqrYzr1M6w7tgimAEXOuW3OuRrgWWB2q2NmA78J3H8euNjMLIQ1dlRHziUsOOfeBva3c8hs4LfO8z7Qx8yyQlPdqenAuYQF51yJc+7DwP0KYAPQ+pvRw+J96eC5hIXAf+vKwMPYwK31VT2d+hnWHYMgG/i4xeNiPvkPoukY51wdcAhICUl1p6Yj5wJwdaDZ/ryZDQhNaZ2uo+caLs4NNO1fNbMxfhdzMoGuhQl4f322FHbvSzvnAmHyvphZtJmtAkqBpc65E74vnfEZ1h2DINK8Agx2zo0DltL8V4L450O8dV3GAz8DXvK3nPaZWSLwAjDPOXfY73rOxEnOJWzeF+dcvXMuH8gBppjZ2GD+vu4YBLuAln8V5wS2tXmMmcUAvYHykFR3ak56Ls65cudcdeDhr4GJIaqts3XkfQsLzrnDjU1759xiINbMUn0uq01mFov3wfmMc+7FNg4Jm/flZOcSTu9LI+fcQWAZML3Vrk79DOuOQbACGGFmQ8wsDm8g5eVWx7wM3Bi4fw3whguMunQxJz2XVv21V+D1jYajl4F/DlylMhU45Jwr8buo02FmmY39tWY2Be//sy73h0agxv8GNjjnHjvBYWHxvnTkXMLofUkzsz6B+z2AzwAbWx3WqZ9hMaf7xK7KOVdnZrcBS/CuunnaObfOzB4CCpxzL+P9g/mdmRXhDfpd51/FJ9bBc7ndzK4A6vDO5SbfCm6Hmf0B76qNVDMrBv4DbxAM59xTwGK8K1SKgKPAv/hT6cl14FyuAb5mZnXAMeC6LvqHxvnADUBhoD8a4F5gIITd+9KRcwmX9yUL+I2ZReOF1ULn3J+D+RmmJSZERCJcd+waEhGRU6AgEBGJcAoCEZEIpyAQEYlwCgIRkQinIBAJocAKmH/2uw6RlhQEIiIRTkEg0gYz+2JgTfhVZvbLwCJglWb2eGCN+L+aWVrg2Hwzez+w8N8fzaxvYPtwM/tLYJGzD81sWODlEwMLBG40s2e66Mq3EkEUBCKtmFkuMBc4P7DwVz3wBaAX3szOMcBbeDOKAX4L/Ftg4b/CFtufAZ4MLHJ2HtC4NMMEYB4wGu+7Js4P8imJtKvbLTEh0gkuxlu8b0Xgj/UeeMsBNwALAsf8HnjRzHoDfZxzbwW2/wZ4zsySgGzn3B8BnHNVAIHX+8A5Vxx4vAoYDPw96GclcgIKApFPMuA3zrl7jtto9p1Wx53u+izVLe7Xo/8PxWfqGhL5pL8C15hZOoCZ9TOzQXj/v1wTOObzwN+dc4eAA2Y2LbD9BuCtwLdkFZvZlYHXiDeznqE8CZGO0l8iIq0459ab2X3A62YWBdQCtwJH8L4k5D68rqK5gafcCDwV+KDfRvMKnTcAvwysGlkLXBvC0xDpMK0+KtJBZlbpnEv0uw6RzqauIRGRCKcWgYhIhFOLQEQkwikIREQinIJARCTCKQhERCKcgkBEJML9f/zn/yE850vRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 2s 4ms/step - loss: 0.2761 - accuracy: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2760693430900574, 0.8971521854400635]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.evaluate(X_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1cf07df3370>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 2s 4ms/step - loss: 0.2642 - accuracy: 0.8942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26424911618232727, 0.8942499756813049]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.evaluate(X_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_prediction_probs = vit_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      7315\n",
      "           1       0.93      0.88      0.90      9224\n",
      "\n",
      "    accuracy                           0.89     16539\n",
      "   macro avg       0.89      0.90      0.89     16539\n",
      "weighted avg       0.90      0.89      0.89     16539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = np.argmax(y_prediction_probs, axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
