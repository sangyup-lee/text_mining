{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D를 적용하지 않은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Korean_movie_reviews_2016.txt', encoding='utf-8') as f:\n",
    "    docs = [doc.strip().split('\\t') for doc in f]\n",
    "    docs = [(doc[0], int(doc[1])) for doc in docs if len(doc) == 2]\n",
    "    texts, labels = zip(*docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [doc.strip().split() for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = []\n",
    "for words in words_list:\n",
    "    total_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52011"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "common_words = [ word for word, count in c.most_common(max_features)]\n",
    "# 빈도를 기준으로 상위 20000개의 단어들만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 대해서 index 생성하기\n",
    "words_dic ={}\n",
    "for index, word in enumerate(common_words):\n",
    "    words_dic[word]=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서를 상위 20000개 단어들에 대해서 index 번호로 표현하기\n",
    "filtered_indexed_words = []\n",
    "for review in words_list:\n",
    "    indexed_words=[]\n",
    "    for word in review:\n",
    "        try:\n",
    "            indexed_words.append(words_dic[word])\n",
    "        except:\n",
    "            pass\n",
    "    filtered_indexed_words.append(indexed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_len = 24\n",
    "X = sequence.pad_sequences(filtered_indexed_words, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148845, 24)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 # 종속변수값이 수 = 2\n",
    "token_emb = 32 # 각 토큰을 32 차원의 벡터로 표현\n",
    "patch_size = 4  # 패치 크기\n",
    "num_patches = (max_len//patch_size)*(token_emb//patch_size)\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  \n",
    "transformer_layers = 2\n",
    "mlp_head_units = [512, 64] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(max_len,))\n",
    "    data = layers.Embedding(max_features+1, token_emb)(inputs)\n",
    "    data = tf.expand_dims(data, -1)\n",
    "    \n",
    "    patches = Patches(patch_size)(data)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    \n",
    "    output = layers.Dense(num_classes, activation='softmax')(features) # 출력층\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output) # 모델 생성\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_classifier = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 24, 32)       640032      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 24, 32, 1)    0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 16)     0           ['tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 48, 64)      4160        ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 48, 64)      128         ['patch_encoder_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 48, 64)      66368       ['layer_normalization_5[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 48, 64)       0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 48, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 48, 128)      8320        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 48, 128)      0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 48, 64)       8256        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 48, 64)       0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 48, 64)       0           ['dropout_8[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 48, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 48, 64)      66368       ['layer_normalization_7[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 48, 64)       0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 48, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 48, 128)      8320        ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 48, 128)      0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 48, 64)       8256        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 48, 64)       0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 48, 64)       0           ['dropout_10[0][0]',             \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 48, 64)      128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3072)         0           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 3072)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 512)          1573376     ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 512)          0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 64)           32832       ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 64)           0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 2)            130         ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,417,058\n",
      "Trainable params: 2,417,058\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfa.optimizers.AdamW(\n",
    "    learning_rate=0.0001, weight_decay=0.0001\n",
    ")\n",
    "vit_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "checkpoint_filepath = \"./checkpoints/checkpoint_vit1\"\n",
    "mc = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', mode='min',\n",
    "                     save_best_only=True,\n",
    "                    save_weights_only=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "524/524 [==============================] - 234s 441ms/step - loss: 0.6724 - accuracy: 0.5754 - val_loss: 0.3661 - val_accuracy: 0.8311\n",
      "Epoch 2/5\n",
      "524/524 [==============================] - 231s 440ms/step - loss: 0.3267 - accuracy: 0.8708 - val_loss: 0.2721 - val_accuracy: 0.8869\n",
      "Epoch 3/5\n",
      "524/524 [==============================] - 231s 440ms/step - loss: 0.2579 - accuracy: 0.9024 - val_loss: 0.2833 - val_accuracy: 0.8846\n",
      "Epoch 3: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = vit_classifier.fit(\n",
    "    x=X_train,\n",
    "    y=y_train_one_hot,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[mc,es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 11s 21ms/step - loss: 0.2788 - accuracy: 0.8888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27877911925315857, 0.8888082504272461]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.evaluate(X_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x207a0bcce80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 10s 20ms/step - loss: 0.2704 - accuracy: 0.8899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2704474925994873, 0.8898966312408447]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.evaluate(X_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 10s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "y_prediction_probs = vit_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      8065\n",
      "           1       0.88      0.91      0.89      8474\n",
      "\n",
      "    accuracy                           0.89     16539\n",
      "   macro avg       0.89      0.89      0.89     16539\n",
      "weighted avg       0.89      0.89      0.89     16539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = np.argmax(y_prediction_probs, axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
