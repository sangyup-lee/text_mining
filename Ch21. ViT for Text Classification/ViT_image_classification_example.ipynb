{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "image_size = 32  # We'll resize input images to this size\n",
    "patch_size = 4  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 2\n",
    "mlp_head_units = [512, 128]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size)\n",
    "    ],\n",
    "    name=\"preprocessing\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "preprocessing.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 32 X 32\n",
      "Patch size: 4 X 4\n",
      "Patches per image: 64\n",
      "Elements per patch: 16\n",
      "The shape of patches:  (1, 64, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALEAAACxCAYAAACLKVzFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFRklEQVR4nO3dPyh9fxzH8Xu/fimDCBvlT7JZkPKnDBaDwoABCwaDjUF3090okoEMLAaLSEaS8mchg6JkEvkzWPwZuDm/4ffb7vvI+Tr3Oq/j+RjfTud+0rNT9+M4J+o4TgRQ9uenFwB8FxFDHhFDHhFDHhFDHhFD3j+f/TAajbL/hsBwHCdqzbkSQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQx4RQ96nT4qHbWBgwJxPTEyk9HPn5+eTZuPj4+axb29vKV1LkHAlhjwihjwihjwihjwihjx2J/7C0dGROX94eDDnFRUVvnzu2NhY0iwvL888dnV11ZxvbW35spYg4UoMeUQMeUQMeUQMeUQMeVHHcdx/GI26//AXy8/PN+fr6+vmvL6+PoWrsT0+Pprzuro6c355eZnK5fjCcZyoNedKDHlEDHlEDHlEDHlEDHnsTnwiOzvbnC8vL5vz1tZWT+dfW1sz52dnZ+a8paUlaVZdXe3pM2dnZ8355uamOQ/SvRbsTiC0iBjyiBjyiBjyiBjy2J34n7UTsbS0ZB7b0dHh6dyvr6/mvKenx5xvbGyY8+Li4m8dG4m477jc39+b876+PnO+vb1tzlOJ3QmEFhFDHhFDHhFD3q/7Ytfb22vOOzs7k2Ze/4ycSCTM+dDQkDl3++Loh3g8bs5jsZin87y8vJjztrY2c76zs+Pp/F7wxQ6hRcSQR8SQR8SQR8SQJ787kZWV5en409NTc15aWvrlc7jtQlgP/ItEIpHp6ekvn9sv5eXl5ryhocGcLy4uejq/287K4OCgp/N4we4EQouIIY+IIY+IIY+IIU/+dQfX19fmPDc3N2WfubCwYM5/YhfCjdsDAp+ensz5ysqKOW9ubjbnZWVl5rywsDBpdnNzYx7rF67EkEfEkEfEkEfEkEfEkBe4eydqamrM+cjIiDlvb28355mZmd9ei9srA6wH+0Uikcjx8fG3PzNodnd3zXljY6M539vbS5o1NTX5shbunUBoETHkETHkETHkETHkBe7eCbdnPXR1dfly/ufnZ3O+v7+fNBsdHTWPdXsdAdx3LVKJKzHkETHkETHkETHkETHkBe7eiY+PD3P+2Tq9GB4eNudzc3O+nD9svN47YcnIyPBlLdw7gdAiYsgjYsgjYsgjYsgL3L0T0aj5BdTz7oTbUx5PTk7MufUfJefn5+axbu+xSLWSkpKkWUFBgS/ndnsHR21trafz9Pf3+7EcT7gSQx4RQx4RQx4RQ96v+7OzF26P9L+9vU3zSv5jPSqgqqrqB1bi/ngC62WXFxcXvnwmf3ZGaBEx5BEx5BEx5BEx5AVudyIej5vzWCyW5pWE09XVlTl/f383526/98PDQ3OeylcbsDuB0CJiyCNiyCNiyCNiyAvc7sTk5KQ5d1tnUVGROe/u7vZtTV+VSCTM+czMTJpX4m5qasqc393dpXkl3rE7gdAiYsgjYsgjYsgjYsgL3O6EVzk5Oea8srIyzStx/6+Ug4ODNK8knNidQGgRMeQRMeQRMeQRMeTJ707g92B3AqFFxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJBHxJD36esOAAVciSGPiCGPiCGPiCGPiCGPiCHvX0o/Pdl6WgF6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "print(\"The shape of patches: \", patches.shape)\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(3, 3))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"), cmap='gray')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    preprocessed = preprocessing(inputs) # 입력 이미지에 대한 전처리 수행, 즉 표준화와 resize\n",
    "    patches = Patches(patch_size)(preprocessed) # 이미지를 패치 단위로 분할\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches) # 각 패치에 대한 임베딩 벡터 생성\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (Sequential)     (None, 32, 32, 1)    3           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " patches_3 (Patches)            (None, None, 16)     0           ['preprocessing[1][0]']          \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 64, 64)      5184        ['patches_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['patch_encoder_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 64, 64)      66368       ['layer_normalization_5[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 64)       0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64, 128)      8320        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 128)      0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64, 64)       8256        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64, 64)       0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64)       0           ['dropout_8[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 64, 64)      66368       ['layer_normalization_7[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 64, 64)       0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 64, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64, 128)      8320        ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64, 128)      0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 64, 64)       8256        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 64, 64)       0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64, 64)       0           ['dropout_10[0][0]',             \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 64, 64)      128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 4096)         0           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 512)          2097664     ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 512)          0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 128)          65664       ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 128)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 10)           1290        ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,336,333\n",
      "Trainable params: 2,336,330\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "checkpoint_filepath = \"./checkpoints/checkpoint_tr1\"\n",
    "mc = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', mode='min', \n",
    "                     save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 179s 425ms/step - loss: 1.2197 - accuracy: 0.5925 - val_loss: 0.3206 - val_accuracy: 0.9138\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 186s 440ms/step - loss: 0.6136 - accuracy: 0.8069 - val_loss: 0.2089 - val_accuracy: 0.9428\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 190s 449ms/step - loss: 0.4486 - accuracy: 0.8645 - val_loss: 0.1523 - val_accuracy: 0.9557\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 193s 457ms/step - loss: 0.3529 - accuracy: 0.8945 - val_loss: 0.1346 - val_accuracy: 0.9657\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 192s 454ms/step - loss: 0.2998 - accuracy: 0.9137 - val_loss: 0.1088 - val_accuracy: 0.9682\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 188s 445ms/step - loss: 0.2605 - accuracy: 0.9241 - val_loss: 0.0910 - val_accuracy: 0.9723\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 180s 426ms/step - loss: 0.2282 - accuracy: 0.9346 - val_loss: 0.0800 - val_accuracy: 0.9777\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 187s 443ms/step - loss: 0.2009 - accuracy: 0.9419 - val_loss: 0.0748 - val_accuracy: 0.9783\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 195s 461ms/step - loss: 0.1821 - accuracy: 0.9462 - val_loss: 0.0649 - val_accuracy: 0.9810\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 198s 468ms/step - loss: 0.1646 - accuracy: 0.9520 - val_loss: 0.0590 - val_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train_one_hot, batch_size=128, epochs=10,\n",
    "                    validation_split=0.1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x253eec44850>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 12s 37ms/step - loss: 0.0713 - accuracy: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07130373269319534, 0.9787999987602234]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
